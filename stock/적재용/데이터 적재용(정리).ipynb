{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import requests\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "# from workalendar.usa import UnitedStates\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "# from bs4 import BeautifulSoup\n",
    "# from tqdm import tqdm\n",
    "# from tqdm.auto import tqdm as tqdm_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = '2021-01-01' # '2022-01-01'\n",
    "# end = '2023-12-31'\n",
    "# symbol = 'AAPL' # \"005930.KS\"\n",
    "# fcast_time = 365 # 365일 예측\n",
    "# freq_option = 'D' \n",
    "\n",
    "def get_stock_data(): #start:str='2021-01-01', end:str='2023-12-31', symbol:str='AAPL'\n",
    "    print(f\"{symbol}의 {start}~{end}기간의 주식데이터입니다\")\n",
    "    total_train_df = yf.download(symbol, start, end)\n",
    "    total_train_df = total_train_df.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "    \n",
    "    return total_train_df\n",
    "\n",
    "def get_stock_data_for_prediction():\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # 아직 day를 단위로만 가능하게 설정되어있음\n",
    "    # 문자열을 datetime 객체로 변환\n",
    "    if fcast_time <= 0 or not isinstance(fcast_time,int):\n",
    "        raise ValueError(\"예측기간은 0보다 큰 정수여야 합니다\")\n",
    "    \n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "    start_date_r = end_date + timedelta(days=1) # 실제 학습에 쓰인 날 마지막 다음날부터하기 위해 end+1\n",
    "    end_date_r = end_date + timedelta(days=fcast_time+1) # fcast_time+1 만큼의 일수를 더함\n",
    "    start_r = start_date_r.strftime('%Y-%m-%d') # 문자로 바꿔줌\n",
    "    end_r = end_date_r.strftime('%Y-%m-%d') # 문자로 바꿔줌\n",
    "\n",
    "    print(f\"예측 확인용 데이터는 {symbol}의 {start_r}~{end_r}기간의 주식데이터입니다\")\n",
    "    df_recent = yf.download(symbol, start=start_r, end=end_r)\n",
    "    df_recent = df_recent.assign(ds = pd.to_datetime(df_recent.index, format = '%Y-%m-%d'))\n",
    "    df_recent = df_recent.assign(y = df_recent['Close'])\n",
    "    \n",
    "    return df_recent\n",
    "    \n",
    "def predict_prophet(total_train_df:pd.DataFrame, column_drop:bool=True):\n",
    "    \n",
    "    # fcast_time 조건 맞게 설정됐는지\n",
    "    if fcast_time <= 0 or not isinstance(fcast_time,int):\n",
    "        raise ValueError(\"예측기간은 0보다 큰 정수여야 합니다\")\n",
    "    \n",
    "    # total_train_df['ds'] = pd.to_datetime(total_train_df.index, format = '%Y-%m-%d')\n",
    "    # total_train_df['y'] = total_train_df['Close']\n",
    "    total_train_df = total_train_df.assign(ds = pd.to_datetime(total_train_df.index, format = '%Y-%m-%d'))\n",
    "    total_train_df = total_train_df.assign(y = total_train_df['Close'])\n",
    "    total_train_df_input = total_train_df[['ds','y']]\n",
    "    # total_train_df = total_train_df.drop(['ds','y'], axis=1)\n",
    "\n",
    "    start_year = int(start.split('-')[0])\n",
    "    range_year = int(end.split('-')[0]) - start_year + 1\n",
    "    us_holidays = make_holidays_df(\n",
    "        year_list=[start_year + i for i in range(range_year)], country='US'\n",
    "    )\n",
    "    us_holidays = us_holidays.assign(lower_window=-1, upper_window=1)\n",
    "    \n",
    "    model_prophet = Prophet(holidays=us_holidays, changepoint_prior_scale=0.3,\n",
    "                            holidays_prior_scale=10.0, n_changepoints=50, \n",
    "                            seasonality_mode='multiplicative', seasonality_prior_scale=0.01).fit(total_train_df_input)\n",
    "\n",
    "    future = model_prophet.make_future_dataframe(periods = fcast_time+3, freq = freq_option)\n",
    "    future = future[(future['ds'].dt.dayofweek < 5)&(~future['ds'].isin(us_holidays['ds']))] # 주말 제외 작업 & # 공휴일 제외 작업\n",
    "\n",
    "    df_forecast = model_prophet.predict(future)\n",
    "\n",
    "    df_forecast.to_csv('prophet_일년짜리예측(ftn).csv', index=False)\n",
    "    if column_drop:\n",
    "        columns_to_drop = ['Christmas Day', 'Christmas Day_lower', 'Christmas Day_upper', 'Christmas Day (observed)', 'Christmas Day (observed)_lower', \n",
    "                    'Christmas Day (observed)_upper', 'Columbus Day', 'Columbus Day_lower', 'Columbus Day_upper', 'Independence Day', \n",
    "                    'Independence Day_lower', 'Independence Day_upper', 'Independence Day (observed)', 'Independence Day (observed)_lower', \n",
    "                    'Independence Day (observed)_upper', 'Juneteenth National Independence Day', 'Juneteenth National Independence Day_lower', \n",
    "                    'Juneteenth National Independence Day_upper', 'Juneteenth National Independence Day (observed)', \n",
    "                    'Juneteenth National Independence Day (observed)_lower', 'Juneteenth National Independence Day (observed)_upper', 'Labor Day', \n",
    "                    'Labor Day_lower', 'Labor Day_upper', 'Martin Luther King Jr. Day', 'Martin Luther King Jr. Day_lower', \n",
    "                    'Martin Luther King Jr. Day_upper', 'Memorial Day', 'Memorial Day_lower', 'Memorial Day_upper', \"New Year's Day\", \n",
    "                    \"New Year's Day_lower\", \"New Year's Day_upper\", \"New Year's Day (observed)\", \"New Year's Day (observed)_lower\", \n",
    "                    \"New Year's Day (observed)_upper\", 'Thanksgiving', 'Thanksgiving_lower', 'Thanksgiving_upper', 'Veterans Day', 'Veterans Day_lower', \n",
    "                    'Veterans Day_upper', 'Veterans Day (observed)', 'Veterans Day (observed)_lower', 'Veterans Day (observed)_upper',\n",
    "                    \"Washington's Birthday\", \"Washington's Birthday_lower\", \"Washington's Birthday_upper\"]\n",
    "\n",
    "        df_forecast = df_forecast.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "    return df_forecast, model_prophet\n",
    "\n",
    "# table_name = 'predict_aapl_2021-01-01to2023-12-31_to365days'\n",
    "def Automatically_create_table_names(predict_status:bool=True) -> str:\n",
    "    if freq_option == 'D': # 후에 딕셔너리 만들어줄까 고민중\n",
    "        datetime = 'days'\n",
    "        \n",
    "    if predict_status == True:\n",
    "        etc='predict_' #''\n",
    "        table_name = f'{etc}{symbol}_{start}to{end}_to{str(fcast_time)}{datetime}'\n",
    "    else:\n",
    "        table_name = f'{symbol}_{start}to{end}'\n",
    "    return table_name\n",
    "\n",
    "\n",
    "def Load_data_in_google_bigquery(df:pd.DataFrame, table_name:str=None, dataset_name:str='stockdata', \n",
    "                                 if_exists_option:str='replace', predict_status:bool=False):\n",
    "    # from google.cloud import bigquery\n",
    "    # from google.cloud.exceptions import NotFound\n",
    "\n",
    "    # 초기화(현재 table_name만)\n",
    "    if table_name is None:\n",
    "        table_name = Automatically_create_table_names(predict_status)\n",
    "    \n",
    "    # BigQuery 클라이언트 생성\n",
    "    client = bigquery.Client()\n",
    "    project_id = client.project #'actual-project-415811'\n",
    "\n",
    "    # 데이터 세트 ID 설정. 이 ID는 '[YOUR_PROJECT_ID].[DATASET_ID]' 형식이어야 합니다.\n",
    "    dataset_id = \"{}.{}\".format(project_id, dataset_name)\n",
    "\n",
    "    # 데이터 세트 참조 생성\n",
    "    dataset_ref = bigquery.DatasetReference.from_string(dataset_id)\n",
    "\n",
    "    try:\n",
    "        # 데이터 세트가 존재하는지 확인\n",
    "        client.get_dataset(dataset_ref)\n",
    "        print(\"Dataset already exists.\")\n",
    "    except NotFound:\n",
    "        # 데이터 세트가 존재하지 않으면, 데이터 세트 생성\n",
    "        dataset = bigquery.Dataset(dataset_id) # 데이터 세트 설정\n",
    "        dataset.location = \"asia-northeast3\" # 데이터 세트의 지역을 설정(서울로 설정)\n",
    "        dataset = client.create_dataset(dataset)  # API request # 데이터 세트 생성\n",
    "        print(\"Created dataset {}.{}\".format(project_id, dataset.dataset_id))\n",
    "\n",
    "    table_id = \"{}.{}\".format(dataset_name,table_name)\n",
    "    \n",
    "    try:\n",
    "        df.to_gbq(destination_table=table_id, project_id=project_id, if_exists=if_exists_option)\n",
    "        print(f\"데이터가 성공적으로 BigQuery {dataset_id}에 {table_name}로 로드되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 로드 중 에러 발생: {e}\")\n",
    "    \n",
    "    \n",
    "def combine_values(df_forecast:pd.DataFrame, df_arima:pd.DataFrame=None):\n",
    "    start = '2021-01-01'\n",
    "    end = '2023-12-31'\n",
    "    symbol = 'AAPL' # \"005930.KS\"\n",
    "    \n",
    "    if df_arima is None:\n",
    "        # 아리마 데이터\n",
    "        df_arima = pd.read_csv('arima_forecast.csv')# 'ARIMA_forecast_2024-01.csv')\n",
    "        df_arima['ds'] = pd.to_datetime(df_arima['ds'])\n",
    "    \n",
    "    # 실제값 가져오기\n",
    "    # get_stock_data() 써도 되지만 적재할 생각은 없는 데이터라 그냥 이것만 해주는게 나았을 것 같아서 안함\n",
    "    df_merge = yf.download(symbol, start, end)\n",
    "    # df_merge = df_merge.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "    df_merge = df_merge.assign(ds = pd.to_datetime(df_merge.index, format = '%Y-%m-%d'))\n",
    "    # df_merge['y'] = df_merge['Close'] # 마찬가지로 굳이 넣어야하나 고민중\n",
    "    df_merge = df_merge.reset_index(drop=True)\n",
    "    df_close = df_merge[['ds','Close']]\n",
    "    \n",
    "    # 예측값으로 부터 필요한 부분 가져와서 실제값을 옆에 붙여주기\n",
    "    df_prophet_concat=df_forecast[['ds','yhat','yhat_lower','yhat_upper']]\n",
    "    df_prophet_concat2 = df_prophet_concat.merge(df_close, on='ds', how='left')\n",
    "    # display(df_prophet_concat2.iloc[-252:-248])\n",
    "    # display(df_prophet_concat2.info())\n",
    "\n",
    "    # 아리마 데이터에 실제값을 옆에 붙여주기\n",
    "    df_arima_concat2 = df_arima.merge(df_close, on='ds', how='left') # df_arima_concat 은 없는게 맞음 애초에 컬럼이 그것들 뿐이라 안만듦\n",
    "    # display(df_arima_concat2.iloc[-22:-18])\n",
    "    df_arima_concat2 = df_arima_concat2.sort_values(by='ds')\n",
    "\n",
    "    # 구분자로 model컬럼 추가\n",
    "    df_prophet_concat2 = df_prophet_concat2.assign(model='prophet')\n",
    "    df_arima_concat2 = df_arima_concat2.assign(model='arima')\n",
    "\n",
    "    # 합치기\n",
    "    combined_data = pd.concat([df_prophet_concat2, df_arima_concat2])\n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL의 2021-01-01~2023-12-31기간의 주식데이터입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확인용 데이터는 AAPL의 2024-01-01~2024-12-31기간의 주식데이터입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:34:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:34:04 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 성공적으로 BigQuery my-personal-project-418208.stockdata에 AAPL_2021-01-01to2023-12-31로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = '2021-01-01' # '2022-01-01'\n",
    "end = '2023-12-31'\n",
    "symbol = 'AAPL' # \"005930.KS\"\n",
    "fcast_time = 365 # 365일 예측\n",
    "freq_option = 'D'\n",
    "\n",
    "total_train_df = get_stock_data()\n",
    "df_recent = get_stock_data_for_prediction()\n",
    "df_forecast, model_prophet = predict_prophet(total_train_df)\n",
    "Load_data_in_google_bigquery(df_forecast) #예측값 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 성공적으로 BigQuery my-personal-project-418208.stockdata에 combined_data로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "combined_data = combine_values(df_forecast=df_forecast) # combined_data.iloc[950:1000] #.iloc[1000:1020]\n",
    "Load_data_in_google_bigquery(combined_data, table_name='combined_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모의계산기용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calculator_data():\n",
    "    start = '2021-01-01'\n",
    "    end = '2024-12-31'\n",
    "    symbol = 'AAPL' # \"005930.KS\"\n",
    "    df = yf.download(symbol, start, end)\n",
    "    # df = df.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "    df['datestamp'] = pd.to_datetime(df.index, format = '%Y-%m-%d')\n",
    "    df_real = df[['datestamp', 'Close']].rename(columns={'Close': 'y'}).reset_index(drop=True)\n",
    "    # display(df_real[-1:])\n",
    "    last_date = df_real['datestamp'].iloc[-1].strftime('%Y-%m-%d')\n",
    "    # last_date = df_real['datestamp'].max().strftime('%Y-%m-%d') # 정렬이 되어있지 않는 경우\n",
    "    # last_date = df_real.tail(1)['datestamp'].iloc[0].strftime('%Y-%m-%d')\n",
    "    \n",
    "    df_predict = pd.read_csv('prophet_일년짜리예측(ftn).csv')\n",
    "    df_predict = df_predict[df_predict['ds']>last_date][['ds','yhat']]\n",
    "    df_predict = df_predict.reset_index(drop=True).rename(columns={'ds': 'datestamp', 'yhat': 'y'})\n",
    "    # df_predict\n",
    "    # df_real.info()\n",
    "    df_predict['datestamp']= pd.to_datetime(df_predict['datestamp'])\n",
    "    # df_predict.info()\n",
    "    calculator_data=pd.concat([df_real, df_predict])\n",
    "    \n",
    "    return calculator_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 성공적으로 BigQuery my-personal-project-418208.stockdata에 calculator_data로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculator_data = create_calculator_data()\n",
    "Load_data_in_google_bigquery(calculator_data, table_name='calculator_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측 평가 지표 기록하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(df_forecast:pd.DataFrame, df_real:pd.DataFrame) -> pd.DataFrame:\n",
    "    import math\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "    \n",
    "    df_forecast = df_forecast.set_index('ds', drop=False)\n",
    "    fc = df_forecast[df_forecast['ds'].isin(df_real['ds'])]['yhat']\n",
    "\n",
    "    mse = mean_squared_error(df_real['y'], fc)\n",
    "    mae = mean_absolute_error(df_real['y'], fc)\n",
    "    rmse = math.sqrt(mean_squared_error(df_real['y'], fc))\n",
    "    mape = mean_absolute_percentage_error(df_real['y'], fc)\n",
    "    r2score = r2_score(df_real['y'],fc)\n",
    "    print(f'mse: {mse}, mae: {mae}, rmse: {rmse}, mape: {mape}, r2_score: {r2score}')\n",
    "    metrics = pd.DataFrame(\n",
    "        {\n",
    "        'MSE': [mse],\n",
    "        'MAE': [mae],\n",
    "        'RMSE': [rmse],\n",
    "        'MAPE_percentage': [mape*100],\n",
    "        'r2_score': [r2score],\n",
    "        }\n",
    "    )\n",
    "    metrics.to_csv('metrics_prophet(ftn).csv', index=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def combine_metrics(metrics_prophet:pd.DataFrame=None, metrics_arima:pd.DataFrame=None) -> pd.DataFrame:\n",
    "    if metrics_prophet is None:\n",
    "        metrics_prophet =pd.read_csv('metrics_prophet(ftn).csv')\n",
    "    if metrics_arima is None:\n",
    "        metrics_arima = pd.read_csv('metrics_arima.csv')\n",
    "    \n",
    "    metrics_prophet = metrics_prophet.assign(model='prophet')\n",
    "    metrics_arima = metrics_arima.assign(model='arima')\n",
    "    metrics_combined = pd.concat([metrics_prophet,metrics_arima])\n",
    "    \n",
    "    return metrics_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL의 2021-01-01~2023-12-31기간의 주식데이터입니다\n",
      "예측 확인용 데이터는 AAPL의 2024-01-01~2024-01-31기간의 주식데이터입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "21:34:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:34:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 23.92446578469712, mae: 4.285436554128468, rmse: 4.8912642317397985, mape: 0.02297579536124007, r2_score: -0.22369331269342552\n",
      "Dataset already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 성공적으로 BigQuery my-personal-project-418208.predict_metrics에 metrics_prophet_aapl로 로드되었습니다.\n",
      "Dataset already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 성공적으로 BigQuery my-personal-project-418208.predict_metrics에 metrics_combined_aapl로 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = '2021-01-01' # '2022-01-01'\n",
    "end = '2023-12-31'\n",
    "symbol = 'AAPL'\n",
    "fcast_time = 30\n",
    "freq_option = 'D'\n",
    "\n",
    "total_train_df_metrics = get_stock_data()\n",
    "df_recent_metrics = get_stock_data_for_prediction()\n",
    "df_forecast_metrics, model_prophet_metrics = predict_prophet(total_train_df_metrics)\n",
    "metrics_prophet = create_metrics(df_forecast=df_forecast_metrics, df_real=df_recent_metrics)\n",
    "Load_data_in_google_bigquery(metrics_prophet, table_name='metrics_prophet_aapl', dataset_name='predict_metrics')\n",
    "metrics_combined = combine_metrics(metrics_prophet=metrics_prophet)\n",
    "Load_data_in_google_bigquery(metrics_prophet, table_name='metrics_combined_aapl', dataset_name='predict_metrics')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
